{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                temp_label = filename.split('.')[0].split('_')[0]\n",
    "                if 'a' in temp_label:\n",
    "                    label ='alcoholic'\n",
    "                else:\n",
    "                    label = 'control'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64)\n",
    "                # with open(rel_path, 'r') as file:\n",
    "                    \n",
    "                #     rwb = list(csv.reader(file, delimiter=\",\"))[0]\n",
    "                #     # scaler = preprocessing.MinMaxScaler()\n",
    "                #     rwb = np.asarray(rwb).astype(np.float64).reshape(-1,1)\n",
    "                #     # print(rwb)\n",
    "                                \n",
    "                temp_data['data'][0] = rwb\n",
    "                temp_data['label'] = label\n",
    "                \n",
    "                # decomp = np.arange(0, 366)\n",
    "                # plt.plot(decomp, df_data)\n",
    "                # plt.xlabel('Dimension Number')\n",
    "                # plt.ylabel('Wavelet Bispectrum Energy')\n",
    "                # plt.show()\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"alcoholic\": 1, \"control\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(path):\n",
    "    # loading extracted feature & label\n",
    "    x = get_dataset(path)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = [\n",
    "        i for i in x[\"data\"]\n",
    "    ]\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = [i for i in x[\"label_map\"]]\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list)).batch(32)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_batch('../smni_cmi_train_rwb2d_256')\n",
    "for i in test:\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "signal = np.load('../Feature/Train/smni_cmi_train_feature_256\\co2a0000364\\co2a0000364_31_feature.npy')\n",
    "\n",
    "# Extract the signal values from the DataFrame\n",
    "\n",
    "# Create a time axis for the signal\n",
    "t = range(len(signal))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the signal\n",
    "ax.plot(t, signal)\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Signal amplitude')\n",
    "ax.set_title('Signal plot')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(61,6)))\n",
    "    model.add(layers.Reshape((61, 6, 1)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=50,\n",
    "    mode='max')\n",
    "    model_path = os.path.join(log_dir,'best_model.h5')\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    return [tensorboard_callback, early_stopping, mc]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "folds = ['train_1', 'test_1', 'epoch_1', 'train_2', 'test_2', 'epoch_2']\n",
    "time_measured = ['Wall_Time_1', 'CPU_Time_1', 'Wall_Time_2', 'CPU_Time_2']\n",
    "epochs = 2000\n",
    "log_dirs = ['logs1/RWB', 'logs2/RWB', 'logs3/RWB', 'logs4/RWB', 'logs5/RWB']\n",
    "train_dir = '../Feature/RWB/Train/smni_cmi_train_feature'\n",
    "test_dir = '../Feature/RWB/Test/smni_cmi_test_feature'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch(train_temp_dir)\n",
    "            test_ds = get_batch(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../smni_cmi_train_rwb2d'\n",
    "test_dir = '../smni_cmi_test_rwb2d'\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 61, 6, 1)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 59, 4, 16)         160       \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 19, 1, 16)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 304)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               156160    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 287,905\n",
      "Trainable params: 287,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "Epoch 1: val_acc improved from -inf to 0.61864, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 2: val_acc improved from 0.61864 to 0.73729, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 3: val_acc did not improve from 0.73729\n",
      "\n",
      "Epoch 4: val_acc did not improve from 0.73729\n",
      "\n",
      "Epoch 5: val_acc did not improve from 0.73729\n",
      "\n",
      "Epoch 6: val_acc improved from 0.73729 to 0.74576, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 7: val_acc did not improve from 0.74576\n",
      "\n",
      "Epoch 8: val_acc improved from 0.74576 to 0.79661, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 9: val_acc did not improve from 0.79661\n",
      "\n",
      "Epoch 10: val_acc did not improve from 0.79661\n",
      "\n",
      "Epoch 11: val_acc improved from 0.79661 to 0.84746, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 12: val_acc did not improve from 0.84746\n",
      "\n",
      "Epoch 13: val_acc improved from 0.84746 to 0.88136, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 14: val_acc did not improve from 0.88136\n",
      "\n",
      "Epoch 15: val_acc did not improve from 0.88136\n",
      "\n",
      "Epoch 16: val_acc improved from 0.88136 to 0.89831, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 17: val_acc did not improve from 0.89831\n",
      "\n",
      "Epoch 18: val_acc did not improve from 0.89831\n",
      "\n",
      "Epoch 19: val_acc did not improve from 0.89831\n",
      "\n",
      "Epoch 20: val_acc improved from 0.89831 to 0.94915, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 21: val_acc improved from 0.94915 to 0.98305, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 22: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 23: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 24: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 25: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 26: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 27: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 28: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 29: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 30: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 31: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 32: val_acc did not improve from 0.98305\n",
      "\n",
      "Epoch 33: val_acc improved from 0.98305 to 1.00000, saving model to ./logsSanbox\\2\\256\\best_model.h5\n",
      "\n",
      "Epoch 34: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 35: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 36: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 37: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 38: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 39: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 40: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 41: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 42: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 43: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 44: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 45: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 46: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 47: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 48: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 49: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 50: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 51: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 52: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 53: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 54: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 55: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 56: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 57: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 58: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 59: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 60: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 61: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 62: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 63: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 64: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 65: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 66: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 67: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 68: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 69: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 70: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 71: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 72: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 73: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 74: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 75: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 76: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 77: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 78: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 79: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 80: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 81: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 82: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 83: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 84: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 85: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 86: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 87: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 88: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 89: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 90: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 91: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 92: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 93: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 94: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 95: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 96: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 97: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 98: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 99: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 100: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 101: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 102: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 103: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 104: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 105: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 106: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 107: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 108: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 109: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 110: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 111: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 112: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 113: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 114: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 115: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 116: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 117: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 118: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 119: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 120: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 121: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 122: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 123: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 124: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 125: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 126: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 127: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 128: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 129: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 130: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 131: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 132: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 133: val_acc did not improve from 1.00000\n",
      "\n",
      "Epoch 134: val_acc did not improve from 1.00000\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.8184 - acc: 0.8721\n"
     ]
    }
   ],
   "source": [
    "fold = 2\n",
    "lag = 256\n",
    "log_dir = './logsSanbox'\n",
    "\n",
    "if fold == 2:\n",
    "    train_dir, test_dir = test_dir, train_dir\n",
    "\n",
    "train_temp_dir = train_dir + '_' + str(lag)\n",
    "test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "train = get_batch(train_temp_dir)\n",
    "test_ds = get_batch(test_temp_dir)\n",
    "\n",
    "train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "train_ds = train.take(train_size)\n",
    "val_ds = train.skip(train_size)\n",
    "\n",
    "log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "cpu_start = time.process_time()\n",
    "wt_start = time.time()\n",
    "\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path), verbose = 0)\n",
    "\n",
    "results = model.evaluate(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "recap.to_csv('./logs/RWB/Recap/recap.csv')\n",
    "training_time.to_csv('./logs/RWB/Recap/training_time.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! tensorboard --logdir logs --port=8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fulky\\OneDrive - UNIVERSITAS INDONESIA\\Documents\\Akademik\\Final Year\\Seminar\\Code\\notebook\n"
     ]
    }
   ],
   "source": [
    "! cd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bispectrum 2D Flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_bispectrum2d_flat(directory):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                temp_label = filename.split('.')[0].split('_')[0]\n",
    "                if 'a' in temp_label:\n",
    "                    label ='alcoholic'\n",
    "                else:\n",
    "                    label = 'control'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64)\n",
    "                # print(rwb)\n",
    "                # with open(rel_path, 'r') as file:\n",
    "                    \n",
    "                #     rwb = list(csv.reader(file, delimiter=\",\"))[0]\n",
    "                #     # scaler = preprocessing.MinMaxScaler()\n",
    "                #     rwb = np.asarray(rwb).astype(np.float64).reshape(-1,1)\n",
    "                #     # print(rwb)\n",
    "                                \n",
    "                temp_data['data'][0] = rwb\n",
    "                temp_data['label'] = label\n",
    "                \n",
    "                # decomp = np.arange(0, 366)\n",
    "                # plt.plot(decomp, df_data)\n",
    "                # plt.xlabel('Dimension Number')\n",
    "                # plt.ylabel('Wavelet Bispectrum Energy')\n",
    "                # plt.show()\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"alcoholic\": 1, \"control\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)\n",
    "    # print(data)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_bispectrum2d_flat(path):\n",
    "    # loading extracted feature & label\n",
    "    x = get_dataset_bispectrum2d_flat(path)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = [\n",
    "        i for i in x[\"data\"]\n",
    "    ]\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = [i for i in x[\"label_map\"]]\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list)).batch(32)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_CNN_bispectrum2d_flat(feature):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(61,feature)))\n",
    "    model.add(layers.Reshape((61, feature, 1)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ANN_bispectrum2d_flat(feature):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(61,feature)))\n",
    "    model.add(layers.Reshape((61, feature, 1)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bispectrum 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_bispectrum2d(directory):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                temp_label = filename.split('.')[0].split('_')[0]\n",
    "                if 'a' in temp_label:\n",
    "                    label ='alcoholic'\n",
    "                else:\n",
    "                    label = 'control'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64)\n",
    "                # print(rwb)\n",
    "                # with open(rel_path, 'r') as file:\n",
    "                    \n",
    "                #     rwb = list(csv.reader(file, delimiter=\",\"))[0]\n",
    "                #     # scaler = preprocessing.MinMaxScaler()\n",
    "                #     rwb = np.asarray(rwb).astype(np.float64).reshape(-1,1)\n",
    "                #     # print(rwb)\n",
    "                                \n",
    "                temp_data['data'][0] = rwb\n",
    "                temp_data['label'] = label\n",
    "                \n",
    "                # decomp = np.arange(0, 366)\n",
    "                # plt.plot(decomp, df_data)\n",
    "                # plt.xlabel('Dimension Number')\n",
    "                # plt.ylabel('Wavelet Bispectrum Energy')\n",
    "                # plt.show()\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"alcoholic\": 1, \"control\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)\n",
    "    # print(data)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_bispectrum2d(path):\n",
    "    # loading extracted feature & label\n",
    "    x = get_dataset_bispectrum2d(path)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = [\n",
    "        i for i in x[\"data\"]\n",
    "    ]\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = [i for i in x[\"label_map\"]]\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list)).batch(32)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_CNN_bispectrum2d(feature):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(61,feature,feature)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ANN_bispectrum2d(feature):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(61,feature,feature)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_rwb(directory):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                temp_label = filename.split('.')[0].split('_')[0]\n",
    "                if 'a' in temp_label:\n",
    "                    label ='alcoholic'\n",
    "                else:\n",
    "                    label = 'control'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64).reshape(-1,1)\n",
    "                # with open(rel_path, 'r') as file:\n",
    "                    \n",
    "                #     rwb = list(csv.reader(file, delimiter=\",\"))[0]\n",
    "                #     # scaler = preprocessing.MinMaxScaler()\n",
    "                #     rwb = np.asarray(rwb).astype(np.float64).reshape(-1,1)\n",
    "                #     # print(rwb)\n",
    "                                \n",
    "                temp_data['data'][0] = rwb\n",
    "                temp_data['label'] = label\n",
    "                \n",
    "                # decomp = np.arange(0, 366)\n",
    "                # plt.plot(decomp, df_data)\n",
    "                # plt.xlabel('Dimension Number')\n",
    "                # plt.ylabel('Wavelet Bispectrum Energy')\n",
    "                # plt.show()\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"alcoholic\": 1, \"control\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_rwb(path):\n",
    "    # loading extracted feature & label\n",
    "    x = get_dataset_rwb(path)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = [\n",
    "        i for i in x[\"data\"]\n",
    "    ]\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = [i for i in x[\"label_map\"]]\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list)).batch(32)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_CNN_rwb():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(366,)))\n",
    "    model.add(layers.Reshape((366, 1)))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling1D(pool_size=3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ANN_rwb():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(366,)))\n",
    "    model.add(layers.Reshape((366, 1)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWB 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_rwb2d(directory):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                temp_label = filename.split('.')[0].split('_')[0]\n",
    "                if 'a' in temp_label:\n",
    "                    label ='alcoholic'\n",
    "                else:\n",
    "                    label = 'control'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64)\n",
    "                # with open(rel_path, 'r') as file:\n",
    "                    \n",
    "                #     rwb = list(csv.reader(file, delimiter=\",\"))[0]\n",
    "                #     # scaler = preprocessing.MinMaxScaler()\n",
    "                #     rwb = np.asarray(rwb).astype(np.float64).reshape(-1,1)\n",
    "                #     # print(rwb)\n",
    "                                \n",
    "                temp_data['data'][0] = rwb\n",
    "                temp_data['label'] = label\n",
    "                \n",
    "                # decomp = np.arange(0, 366)\n",
    "                # plt.plot(decomp, df_data)\n",
    "                # plt.xlabel('Dimension Number')\n",
    "                # plt.ylabel('Wavelet Bispectrum Energy')\n",
    "                # plt.show()\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"alcoholic\": 1, \"control\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_rwb2d(path):\n",
    "    # loading extracted feature & label\n",
    "    x = get_dataset_rwb2d(path)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = [\n",
    "        i for i in x[\"data\"]\n",
    "    ]\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = [i for i in x[\"label_map\"]]\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list)).batch(32)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_CNN_rwb_2d():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(61,6)))\n",
    "    model.add(layers.Reshape((61, 6, 1)))\n",
    "\n",
    "    model.add(layers.Conv2D(filters=16, kernel_size=(3,3), activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling2D(pool_size=3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ANN_rwb_2d():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(61,6)))\n",
    "    model.add(layers.Reshape((61, 6, 1)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bispectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_bispectrum(directory):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                temp_label = filename.split('.')[0].split('_')[0]\n",
    "                if 'a' in temp_label:\n",
    "                    label ='alcoholic'\n",
    "                else:\n",
    "                    label = 'control'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64).reshape(-1,1)\n",
    "                # print(rwb)\n",
    "                # with open(rel_path, 'r') as file:\n",
    "                    \n",
    "                #     rwb = list(csv.reader(file, delimiter=\",\"))[0]\n",
    "                #     # scaler = preprocessing.MinMaxScaler()\n",
    "                #     rwb = np.asarray(rwb).astype(np.float64).reshape(-1,1)\n",
    "                #     # print(rwb)\n",
    "                                \n",
    "                temp_data['data'][0] = rwb\n",
    "                temp_data['label'] = label\n",
    "                \n",
    "                # decomp = np.arange(0, 366)\n",
    "                # plt.plot(decomp, df_data)\n",
    "                # plt.xlabel('Dimension Number')\n",
    "                # plt.ylabel('Wavelet Bispectrum Energy')\n",
    "                # plt.show()\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"alcoholic\": 1, \"control\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)\n",
    "    # print(data)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_bispectrum(path):\n",
    "    # loading extracted feature & label\n",
    "    x = get_dataset_bispectrum(path)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = [\n",
    "        i for i in x[\"data\"]\n",
    "    ]\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = [i for i in x[\"label_map\"]]\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list)).batch(32)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_CNN_bispectrum(feature):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(feature,)))\n",
    "    model.add(layers.Reshape((feature, 1)))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling1D(pool_size=3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ANN_bispectrum(feature):\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(feature,)))\n",
    "    model.add(layers.Reshape((feature, 1)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='acc',\n",
    "    patience=50,\n",
    "    mode='max')\n",
    "    model_path = os.path.join(log_dir,'best_model.h5')\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint(model_path, monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    return [tensorboard_callback, early_stopping, mc]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "folds = ['train_1', 'test_1', 'epoch_1', 'train_2', 'test_2', 'epoch_2']\n",
    "time_measured = ['Wall_Time_1', 'CPU_Time_1', 'Wall_Time_2', 'CPU_Time_2']\n",
    "epochs = 2000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RWB 2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../Feature/RWB2D/Train/smni_cmi_train_rwb2d_256'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m train_temp_dir \u001b[39m=\u001b[39m train_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[0;32m     13\u001b[0m test_temp_dir \u001b[39m=\u001b[39m test_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[1;32m---> 15\u001b[0m train \u001b[39m=\u001b[39m get_batch_rwb2d(train_temp_dir)\n\u001b[0;32m     16\u001b[0m test_ds \u001b[39m=\u001b[39m get_batch_rwb2d(test_temp_dir)\n\u001b[0;32m     18\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(train\u001b[39m.\u001b[39mas_numpy_iterator()))\u001b[39m*\u001b[39m\u001b[39m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m, in \u001b[0;36mget_batch_rwb2d\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_batch_rwb2d\u001b[39m(path):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# loading extracted feature & label\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     x \u001b[39m=\u001b[39m get_dataset_rwb2d(path)\n\u001b[0;32m      5\u001b[0m     scaler \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mMinMaxScaler()\n\u001b[0;32m      7\u001b[0m     series_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m         i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mget_dataset_rwb2d\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset_rwb2d\u001b[39m(directory):\n\u001b[0;32m      2\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m foldername \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(directory):\n\u001b[0;32m      4\u001b[0m         folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, foldername)\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(folder):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Feature/RWB2D/Train/smni_cmi_train_rwb2d_256'"
     ]
    }
   ],
   "source": [
    "log_dirs = ['logs/CNN_RWB2D','logs1/CNN_RWB2D', 'logs2/CNN_RWB2D', 'logs3/CNN_RWB2D', 'logs4/CNN_RWB2D', 'logs5/CNN_RWB2D', 'logs6/CNN_RWB2D', 'logs7/CNN_RWB2D', 'logs8/CNN_RWB2D', 'logs9/CNN_RWB2D']\n",
    "train_dir = '../Feature/RWB2D/Train/smni_cmi_train_rwb2d'\n",
    "test_dir = '../Feature/RWB2D/Test/smni_cmi_test_rwb2d'\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_rwb2d(train_temp_dir)\n",
    "            test_ds = get_batch_rwb2d(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_CNN_rwb_2d()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../Feature/RWB2D/Train/smni_cmi_train_rwb2d_256'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m train_temp_dir \u001b[39m=\u001b[39m train_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[0;32m     13\u001b[0m test_temp_dir \u001b[39m=\u001b[39m test_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[1;32m---> 15\u001b[0m train \u001b[39m=\u001b[39m get_batch_rwb2d(train_temp_dir)\n\u001b[0;32m     16\u001b[0m test_ds \u001b[39m=\u001b[39m get_batch_rwb2d(test_temp_dir)\n\u001b[0;32m     18\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(train\u001b[39m.\u001b[39mas_numpy_iterator()))\u001b[39m*\u001b[39m\u001b[39m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m, in \u001b[0;36mget_batch_rwb2d\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_batch_rwb2d\u001b[39m(path):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# loading extracted feature & label\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     x \u001b[39m=\u001b[39m get_dataset_rwb2d(path)\n\u001b[0;32m      5\u001b[0m     scaler \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mMinMaxScaler()\n\u001b[0;32m      7\u001b[0m     series_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m         i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mget_dataset_rwb2d\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset_rwb2d\u001b[39m(directory):\n\u001b[0;32m      2\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m foldername \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(directory):\n\u001b[0;32m      4\u001b[0m         folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, foldername)\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(folder):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Feature/RWB2D/Train/smni_cmi_train_rwb2d_256'"
     ]
    }
   ],
   "source": [
    "log_dirs = ['logs/ANN_RWB2D','logs1/ANN_RWB2D', 'logs2/ANN_RWB2D', 'logs3/ANN_RWB2D', 'logs4/ANN_RWB2D', 'logs5/ANN_RWB2D', 'logs6/ANN_RWB2D', 'logs7/ANN_RWB2D', 'logs8/ANN_RWB2D', 'logs9/ANN_RWB2D']\n",
    "train_dir = '../Feature/RWB2D/Train/smni_cmi_train_rwb2d'\n",
    "test_dir = '../Feature/RWB2D/Test/smni_cmi_test_rwb2d'\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_rwb2d(train_temp_dir)\n",
    "            test_ds = get_batch_rwb2d(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_ANN_rwb_2d()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bispectrum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../Feature/Bispectrum1D/Train/smni_cmi_train_bispectrum_256'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m train_temp_dir \u001b[39m=\u001b[39m train_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[0;32m     13\u001b[0m test_temp_dir \u001b[39m=\u001b[39m test_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[1;32m---> 15\u001b[0m train \u001b[39m=\u001b[39m get_batch_bispectrum(train_temp_dir)\n\u001b[0;32m     16\u001b[0m test_ds \u001b[39m=\u001b[39m get_batch_bispectrum(test_temp_dir)\n\u001b[0;32m     18\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(train\u001b[39m.\u001b[39mas_numpy_iterator()))\u001b[39m*\u001b[39m\u001b[39m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m, in \u001b[0;36mget_batch_bispectrum\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_batch_bispectrum\u001b[39m(path):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# loading extracted feature & label\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     x \u001b[39m=\u001b[39m get_dataset_bispectrum(path)\n\u001b[0;32m      5\u001b[0m     scaler \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mMinMaxScaler()\n\u001b[0;32m      7\u001b[0m     series_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m         i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m, in \u001b[0;36mget_dataset_bispectrum\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset_bispectrum\u001b[39m(directory):\n\u001b[0;32m      2\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m foldername \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(directory):\n\u001b[0;32m      4\u001b[0m         folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, foldername)\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(folder):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Feature/Bispectrum1D/Train/smni_cmi_train_bispectrum_256'"
     ]
    }
   ],
   "source": [
    "log_dirs = ['logs/CNN_Bispectrum','logs1/CNN_Bispectrum', 'logs2/CNN_Bispectrum', 'logs3/CNN_Bispectrum', 'logs4/CNN_Bispectrum', 'logs5/CNN_Bispectrum', 'logs6/CNN_Bispectrum', 'logs7/CNN_Bispectrum', 'logs8/CNN_Bispectrum', 'logs9/CNN_Bispectrum']\n",
    "train_dir = '../Feature/Bispectrum1D/Train/smni_cmi_train_bispectrum'\n",
    "test_dir = '../Feature/Bispectrum1D/Test/smni_cmi_test_bispectrum'\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured) \n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_bispectrum(train_temp_dir)\n",
    "            test_ds = get_batch_bispectrum(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_CNN_bispectrum([i[0].shape[1] for i in train][0])\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path), verbose = 0)\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        print('test')\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../Feature/Bispectrum1D/Train/smni_cmi_train_bispectrum_256'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m train_temp_dir \u001b[39m=\u001b[39m train_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[0;32m     13\u001b[0m test_temp_dir \u001b[39m=\u001b[39m test_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[1;32m---> 15\u001b[0m train \u001b[39m=\u001b[39m get_batch_bispectrum(train_temp_dir)\n\u001b[0;32m     16\u001b[0m test_ds \u001b[39m=\u001b[39m get_batch_bispectrum(test_temp_dir)\n\u001b[0;32m     18\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(train\u001b[39m.\u001b[39mas_numpy_iterator()))\u001b[39m*\u001b[39m\u001b[39m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m, in \u001b[0;36mget_batch_bispectrum\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_batch_bispectrum\u001b[39m(path):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# loading extracted feature & label\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     x \u001b[39m=\u001b[39m get_dataset_bispectrum(path)\n\u001b[0;32m      5\u001b[0m     scaler \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mMinMaxScaler()\n\u001b[0;32m      7\u001b[0m     series_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m         i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m, in \u001b[0;36mget_dataset_bispectrum\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset_bispectrum\u001b[39m(directory):\n\u001b[0;32m      2\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m foldername \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(directory):\n\u001b[0;32m      4\u001b[0m         folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, foldername)\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(folder):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Feature/Bispectrum1D/Train/smni_cmi_train_bispectrum_256'"
     ]
    }
   ],
   "source": [
    "log_dirs = ['logs/ANN_Bispectrum','logs1/ANN_Bispectrum', 'logs2/ANN_Bispectrum', 'logs3/ANN_Bispectrum', 'logs4/ANN_Bispectrum', 'logs5/ANN_Bispectrum', 'logs6/ANN_Bispectrum', 'logs7/ANN_Bispectrum', 'logs8/ANN_Bispectrum', 'logs9/ANN_Bispectrum']\n",
    "train_dir = '../Feature/Bispectrum1D/Train/smni_cmi_train_bispectrum'\n",
    "test_dir = '../Feature/Bispectrum1D/Test/smni_cmi_test_bispectrum'\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured) \n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_bispectrum(train_temp_dir)\n",
    "            test_ds = get_batch_bispectrum(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_ANN_bispectrum([i[0].shape[1] for i in train][0])\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path), verbose = 0)\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        print('test')\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RWB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '../Feature/RWB/Train/smni_cmi_train_feature_256'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m train_temp_dir \u001b[39m=\u001b[39m train_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[0;32m     14\u001b[0m test_temp_dir \u001b[39m=\u001b[39m test_dir \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(lag)\n\u001b[1;32m---> 16\u001b[0m train \u001b[39m=\u001b[39m get_batch_rwb(train_temp_dir)\n\u001b[0;32m     17\u001b[0m test_ds \u001b[39m=\u001b[39m get_batch_rwb(test_temp_dir)\n\u001b[0;32m     19\u001b[0m train_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mlist\u001b[39m(train\u001b[39m.\u001b[39mas_numpy_iterator()))\u001b[39m*\u001b[39m\u001b[39m0.8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36mget_batch_rwb\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_batch_rwb\u001b[39m(path):\n\u001b[0;32m      2\u001b[0m     \u001b[39m# loading extracted feature & label\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     x \u001b[39m=\u001b[39m get_dataset_rwb(path)\n\u001b[0;32m      5\u001b[0m     scaler \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39mMinMaxScaler()\n\u001b[0;32m      7\u001b[0m     series_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m         i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      9\u001b[0m     ]\n",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m, in \u001b[0;36mget_dataset_rwb\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataset_rwb\u001b[39m(directory):\n\u001b[0;32m      2\u001b[0m     data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 3\u001b[0m     \u001b[39mfor\u001b[39;00m foldername \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(directory):\n\u001b[0;32m      4\u001b[0m         folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(directory, foldername)\n\u001b[0;32m      5\u001b[0m         \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(folder):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '../Feature/RWB/Train/smni_cmi_train_feature_256'"
     ]
    }
   ],
   "source": [
    "log_dirs = ['logs/CNN_RWB','logs1/CNN_RWB', 'logs2/CNN_RWB', 'logs3/CNN_RWB', 'logs4/CNN_RWB', 'logs5/CNN_RWB', 'logs6/CNN_RWB', 'logs7/CNN_RWB', 'logs8/CNN_RWB', 'logs9/CNN_RWB']\n",
    "train_dir = '../Feature/RWB/Train/smni_cmi_train_feature'\n",
    "test_dir = '../Feature/RWB/Test/smni_cmi_test_feature'\n",
    "\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_rwb(train_temp_dir)\n",
    "            test_ds = get_batch_rwb(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_CNN_rwb()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = ['logs/ANN_RWB','logs1/ANN_RWB', 'logs2/ANN_RWB', 'logs3/ANN_RWB', 'logs4/ANN_RWB', 'logs5/ANN_RWB', 'logs6/ANN_RWB', 'logs7/ANN_RWB', 'logs8/ANN_RWB', 'logs9/ANN_RWB']\n",
    "train_dir = '../Feature/RWB/Train/smni_cmi_train_feature'\n",
    "test_dir = '../Feature/RWB/Test/smni_cmi_test_feature'\n",
    "\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured)\n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_rwb(train_temp_dir)\n",
    "            test_ds = get_batch_rwb(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_ANN_rwb()\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bispectrum 2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(11, 61, 5, 5), dtype=float64, numpy=\n",
      "array([[[[2.50395398e+01, 2.75768737e+01, 8.34577261e+01,\n",
      "          1.09540747e+02, 5.98574576e+02],\n",
      "         [2.75768737e+01, 7.56022694e+01, 2.77824430e+02,\n",
      "          4.80120924e+02, 4.09765366e+03],\n",
      "         [8.34577261e+01, 2.77824430e+02, 1.68356646e+03,\n",
      "          4.69050982e+03, 4.14603556e+04],\n",
      "         [1.09540747e+02, 4.80120924e+02, 4.69050982e+03,\n",
      "          1.19398599e+04, 5.69054732e+04],\n",
      "         [5.98574576e+02, 4.09765366e+03, 4.14603556e+04,\n",
      "          5.69054732e+04, 1.28773786e+06]],\n",
      "\n",
      "        [[9.28411317e+00, 1.02202270e+01, 4.01709799e+01,\n",
      "          3.43116634e+01, 1.56196907e+02],\n",
      "         [1.02202270e+01, 5.37734775e+01, 2.27115255e+02,\n",
      "          4.97654701e+02, 8.94874746e+03],\n",
      "         [4.01709799e+01, 2.27115255e+02, 4.52311257e+03,\n",
      "          1.24605889e+04, 9.03842245e+04],\n",
      "         [3.43116634e+01, 4.97654701e+02, 1.24605889e+04,\n",
      "          2.07727548e+04, 6.19009804e+04],\n",
      "         [1.56196907e+02, 8.94874746e+03, 9.03842245e+04,\n",
      "          6.19009804e+04, 1.00849698e+06]],\n",
      "\n",
      "        [[2.36124801e+00, 4.34864134e+00, 1.57528294e+01,\n",
      "          1.17055064e+01, 3.61917750e+01],\n",
      "         [4.34864134e+00, 4.21632669e+01, 1.82566125e+02,\n",
      "          3.36657193e+02, 3.14903871e+03],\n",
      "         [1.57528294e+01, 1.82566125e+02, 3.52386400e+03,\n",
      "          6.05520205e+03, 2.88102740e+04],\n",
      "         [1.17055064e+01, 3.36657193e+02, 6.05520205e+03,\n",
      "          8.19591016e+03, 1.47320462e+04],\n",
      "         [3.61917750e+01, 3.14903871e+03, 2.88102740e+04,\n",
      "          1.47320462e+04, 9.46549632e+04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.51591002e-01, 5.93925736e-01, 2.41569485e+00,\n",
      "          2.74167296e+00, 6.92016870e+00],\n",
      "         [5.93925736e-01, 3.89864403e+00, 1.58235876e+01,\n",
      "          2.59113288e+01, 2.68743690e+02],\n",
      "         [2.41569485e+00, 1.58235876e+01, 2.86349301e+02,\n",
      "          7.42367561e+02, 2.56694448e+03],\n",
      "         [2.74167296e+00, 2.59113288e+01, 7.42367561e+02,\n",
      "          1.28001382e+03, 3.81376431e+03],\n",
      "         [6.92016870e+00, 2.68743690e+02, 2.56694448e+03,\n",
      "          3.81376431e+03, 1.50178097e+04]],\n",
      "\n",
      "        [[1.98805884e-01, 3.62742193e-01, 2.27726347e+00,\n",
      "          1.92178218e+00, 5.21483250e+00],\n",
      "         [3.62742193e-01, 2.07894744e+00, 1.26385368e+01,\n",
      "          2.18089548e+01, 1.92619770e+02],\n",
      "         [2.27726347e+00, 1.26385368e+01, 4.20470920e+02,\n",
      "          9.57803490e+02, 4.63450669e+03],\n",
      "         [1.92178218e+00, 2.18089548e+01, 9.57803490e+02,\n",
      "          1.68576340e+03, 4.42077619e+03],\n",
      "         [5.21483250e+00, 1.92619770e+02, 4.63450669e+03,\n",
      "          4.42077619e+03, 1.76309634e+04]],\n",
      "\n",
      "        [[8.79706294e-02, 1.43848738e-01, 7.11011529e-01,\n",
      "          6.52738992e-01, 2.16140314e+00],\n",
      "         [1.43848738e-01, 6.15814518e-01, 3.19979268e+00,\n",
      "          3.25044252e+00, 2.90800510e+01],\n",
      "         [7.11011529e-01, 3.19979268e+00, 4.26904122e+01,\n",
      "          9.03489746e+01, 4.92748270e+02],\n",
      "         [6.52738992e-01, 3.25044252e+00, 9.03489746e+01,\n",
      "          1.50954260e+02, 5.14556490e+02],\n",
      "         [2.16140314e+00, 2.90800510e+01, 4.92748270e+02,\n",
      "          5.14556490e+02, 2.87138583e+03]]],\n",
      "\n",
      "\n",
      "       [[[2.69131601e+01, 3.17385080e+01, 7.06943311e+01,\n",
      "          1.17979645e+02, 1.04319454e+03],\n",
      "         [3.17385080e+01, 9.32031880e+01, 2.41261451e+02,\n",
      "          5.32163694e+02, 1.03663139e+04],\n",
      "         [7.06943311e+01, 2.41261451e+02, 1.17307797e+03,\n",
      "          4.68192870e+03, 4.70408706e+04],\n",
      "         [1.17979645e+02, 5.32163694e+02, 4.68192870e+03,\n",
      "          9.77301111e+03, 9.30803163e+04],\n",
      "         [1.04319454e+03, 1.03663139e+04, 4.70408706e+04,\n",
      "          9.30803163e+04, 6.61410108e+06]],\n",
      "\n",
      "        [[3.67480402e+01, 4.49160737e+01, 1.17090154e+02,\n",
      "          1.32304572e+02, 9.40612025e+02],\n",
      "         [4.49160737e+01, 2.11914784e+02, 6.53522325e+02,\n",
      "          1.13870986e+03, 2.77029079e+04],\n",
      "         [1.17090154e+02, 6.53522325e+02, 6.42054453e+03,\n",
      "          1.62237011e+04, 1.32937592e+05],\n",
      "         [1.32304572e+02, 1.13870986e+03, 1.62237011e+04,\n",
      "          2.41986074e+04, 1.36877964e+05],\n",
      "         [9.40612025e+02, 2.77029079e+04, 1.32937592e+05,\n",
      "          1.36877964e+05, 5.21442094e+06]],\n",
      "\n",
      "        [[2.71233162e+00, 4.75642976e+00, 1.01017446e+01,\n",
      "          1.17265096e+01, 4.81283556e+01],\n",
      "         [4.75642976e+00, 4.87779467e+01, 1.38255889e+02,\n",
      "          4.88033079e+02, 6.79272459e+03],\n",
      "         [1.01017446e+01, 1.38255889e+02, 2.17104129e+03,\n",
      "          5.27900670e+03, 1.97062508e+04],\n",
      "         [1.17265096e+01, 4.88033079e+02, 5.27900670e+03,\n",
      "          5.48475126e+03, 2.45669968e+04],\n",
      "         [4.81283556e+01, 6.79272459e+03, 1.97062508e+04,\n",
      "          2.45669968e+04, 3.09297481e+05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.52321782e-01, 3.16525143e-01, 1.06614503e+00,\n",
      "          1.84871127e+00, 5.59484003e+00],\n",
      "         [3.16525143e-01, 1.93601714e+00, 7.86921037e+00,\n",
      "          2.31188618e+01, 1.56153750e+02],\n",
      "         [1.06614503e+00, 7.86921037e+00, 8.04657584e+01,\n",
      "          3.26890844e+02, 1.24592311e+03],\n",
      "         [1.84871127e+00, 2.31188618e+01, 3.26890844e+02,\n",
      "          7.94397522e+02, 3.19640073e+03],\n",
      "         [5.59484003e+00, 1.56153750e+02, 1.24592311e+03,\n",
      "          3.19640073e+03, 2.34842135e+04]],\n",
      "\n",
      "        [[1.37330933e-01, 3.30761383e-01, 1.14315130e+00,\n",
      "          1.54213459e+00, 5.80204045e+00],\n",
      "         [3.30761383e-01, 2.68550626e+00, 1.02373111e+01,\n",
      "          2.05572501e+01, 2.45718786e+02],\n",
      "         [1.14315130e+00, 1.02373111e+01, 1.01401950e+02,\n",
      "          3.86977910e+02, 1.80829937e+03],\n",
      "         [1.54213459e+00, 2.05572501e+01, 3.86977910e+02,\n",
      "          7.03430437e+02, 2.42977900e+03],\n",
      "         [5.80204045e+00, 2.45718786e+02, 1.80829937e+03,\n",
      "          2.42977900e+03, 2.37887742e+04]],\n",
      "\n",
      "        [[4.41280867e-02, 9.22847829e-02, 3.08284258e-01,\n",
      "          4.29449766e-01, 1.84004523e+00],\n",
      "         [9.22847829e-02, 3.36792628e-01, 1.08652476e+00,\n",
      "          2.09430163e+00, 2.44171537e+01],\n",
      "         [3.08284258e-01, 1.08652476e+00, 9.62009692e+00,\n",
      "          3.10789745e+01, 1.79320585e+02],\n",
      "         [4.29449766e-01, 2.09430163e+00, 3.10789745e+01,\n",
      "          6.51377536e+01, 3.07636902e+02],\n",
      "         [1.84004523e+00, 2.44171537e+01, 1.79320585e+02,\n",
      "          3.07636902e+02, 4.48590494e+03]]],\n",
      "\n",
      "\n",
      "       [[[9.94253561e-01, 1.19374328e+00, 4.31926492e+00,\n",
      "          4.47530518e+00, 1.42123641e+01],\n",
      "         [1.19374328e+00, 6.79723111e+00, 2.76255849e+01,\n",
      "          5.64867411e+01, 5.91594927e+02],\n",
      "         [4.31926492e+00, 2.76255849e+01, 4.82264633e+02,\n",
      "          1.32028376e+03, 5.15740107e+03],\n",
      "         [4.47530518e+00, 5.64867411e+01, 1.32028376e+03,\n",
      "          1.83012803e+03, 4.94910911e+03],\n",
      "         [1.42123641e+01, 5.91594927e+02, 5.15740107e+03,\n",
      "          4.94910911e+03, 3.30099601e+04]],\n",
      "\n",
      "        [[1.51181253e+00, 2.45715083e+00, 9.00517392e+00,\n",
      "          6.42560380e+00, 1.36598160e+01],\n",
      "         [2.45715083e+00, 2.53030282e+01, 1.08111963e+02,\n",
      "          2.57401063e+02, 2.19522843e+03],\n",
      "         [9.00517392e+00, 1.08111963e+02, 4.26928616e+03,\n",
      "          5.66552987e+03, 1.81430378e+04],\n",
      "         [6.42560380e+00, 2.57401063e+02, 5.66552987e+03,\n",
      "          7.28771790e+03, 1.16244081e+04],\n",
      "         [1.36598160e+01, 2.19522843e+03, 1.81430378e+04,\n",
      "          1.16244081e+04, 3.08884941e+04]],\n",
      "\n",
      "        [[2.36189525e+00, 3.04832820e+00, 8.98561208e+00,\n",
      "          5.55050696e+00, 1.48518682e+01],\n",
      "         [3.04832820e+00, 2.70916578e+01, 8.71115300e+01,\n",
      "          1.76489461e+02, 2.23259520e+03],\n",
      "         [8.98561208e+00, 8.71115300e+01, 2.61879334e+03,\n",
      "          3.94655818e+03, 1.18914892e+04],\n",
      "         [5.55050696e+00, 1.76489461e+02, 3.94655818e+03,\n",
      "          2.98378848e+03, 5.28654697e+03],\n",
      "         [1.48518682e+01, 2.23259520e+03, 1.18914892e+04,\n",
      "          5.28654697e+03, 2.22940995e+04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[5.14155037e-02, 1.02121053e-01, 4.56451569e-01,\n",
      "          6.99753201e-01, 1.30322719e+00],\n",
      "         [1.02121053e-01, 9.16178907e-01, 4.53224208e+00,\n",
      "          1.19715993e+01, 8.29343369e+01],\n",
      "         [4.56451569e-01, 4.53224208e+00, 8.51181966e+01,\n",
      "          3.74331121e+02, 9.20614268e+02],\n",
      "         [6.99753201e-01, 1.19715993e+01, 3.74331121e+02,\n",
      "          8.22165560e+02, 2.41829858e+03],\n",
      "         [1.30322719e+00, 8.29343369e+01, 9.20614268e+02,\n",
      "          2.41829858e+03, 5.94731683e+03]],\n",
      "\n",
      "        [[1.08766838e-01, 2.20955378e-01, 9.92217113e-01,\n",
      "          1.39832869e+00, 2.38570209e+00],\n",
      "         [2.20955378e-01, 1.51487071e+00, 8.23163797e+00,\n",
      "          1.99332685e+01, 1.62376514e+02],\n",
      "         [9.92217113e-01, 8.23163797e+00, 2.40441784e+02,\n",
      "          6.93023914e+02, 1.75459058e+03],\n",
      "         [1.39832869e+00, 1.99332685e+01, 6.93023914e+02,\n",
      "          1.30423263e+03, 4.07928202e+03],\n",
      "         [2.38570209e+00, 1.62376514e+02, 1.75459058e+03,\n",
      "          4.07928202e+03, 8.40695413e+03]],\n",
      "\n",
      "        [[4.88408887e-02, 9.46493141e-02, 3.56365791e-01,\n",
      "          5.42312051e-01, 9.86889204e-01],\n",
      "         [9.46493141e-02, 2.95072830e-01, 1.12933600e+00,\n",
      "          2.24380484e+00, 1.11870872e+01],\n",
      "         [3.56365791e-01, 1.12933600e+00, 1.12167649e+01,\n",
      "          4.19540266e+01, 9.99020774e+01],\n",
      "         [5.42312051e-01, 2.24380484e+00, 4.19540266e+01,\n",
      "          7.99671609e+01, 2.48584281e+02],\n",
      "         [9.86889204e-01, 1.11870872e+01, 9.99020774e+01,\n",
      "          2.48584281e+02, 6.90725461e+02]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[1.27359549e+01, 1.44817902e+01, 3.38584840e+01,\n",
      "          3.80412474e+01, 1.25726125e+02],\n",
      "         [1.44817902e+01, 5.55898969e+01, 1.67903158e+02,\n",
      "          2.88421921e+02, 2.53717612e+03],\n",
      "         [3.38584840e+01, 1.67903158e+02, 1.41135496e+03,\n",
      "          2.55543236e+03, 9.63922887e+03],\n",
      "         [3.80412474e+01, 2.88421921e+02, 2.55543236e+03,\n",
      "          3.79359726e+03, 1.10031610e+04],\n",
      "         [1.25726125e+02, 2.53717612e+03, 9.63922887e+03,\n",
      "          1.10031610e+04, 8.48434429e+04]],\n",
      "\n",
      "        [[8.13009934e+00, 9.09726866e+00, 2.50191132e+01,\n",
      "          2.10461066e+01, 5.74290236e+01],\n",
      "         [9.09726866e+00, 6.75151262e+01, 2.29263922e+02,\n",
      "          5.05106730e+02, 5.49906061e+03],\n",
      "         [2.50191132e+01, 2.29263922e+02, 3.66571020e+03,\n",
      "          8.55165604e+03, 2.63351382e+04],\n",
      "         [2.10461066e+01, 5.05106730e+02, 8.55165604e+03,\n",
      "          9.50042924e+03, 1.72521700e+04],\n",
      "         [5.74290236e+01, 5.49906061e+03, 2.63351382e+04,\n",
      "          1.72521700e+04, 8.81551782e+04]],\n",
      "\n",
      "        [[2.93269943e+01, 3.60347770e+01, 9.61911472e+01,\n",
      "          5.97285737e+01, 1.54099914e+02],\n",
      "         [3.60347770e+01, 2.10216930e+02, 7.20328602e+02,\n",
      "          9.25365209e+02, 6.08357918e+03],\n",
      "         [9.61911472e+01, 7.20328602e+02, 7.12091412e+03,\n",
      "          1.06821759e+04, 3.05536442e+04],\n",
      "         [5.97285737e+01, 9.25365209e+02, 1.06821759e+04,\n",
      "          7.62566745e+03, 1.01486996e+04],\n",
      "         [1.54099914e+02, 6.08357918e+03, 3.05536442e+04,\n",
      "          1.01486996e+04, 4.56496673e+04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.51534821e-01, 8.97129654e-01, 3.02842031e+00,\n",
      "          3.44476613e+00, 6.92183532e+00],\n",
      "         [8.97129654e-01, 3.85371716e+00, 1.43748606e+01,\n",
      "          2.40927474e+01, 1.18403969e+02],\n",
      "         [3.02842031e+00, 1.43748606e+01, 1.61436736e+02,\n",
      "          4.23151029e+02, 1.09510263e+03],\n",
      "         [3.44476613e+00, 2.40927474e+01, 4.23151029e+02,\n",
      "          6.95456458e+02, 1.53668844e+03],\n",
      "         [6.92183532e+00, 1.18403969e+02, 1.09510263e+03,\n",
      "          1.53668844e+03, 3.92571203e+03]],\n",
      "\n",
      "        [[4.19500684e-01, 7.97942345e-01, 2.68371322e+00,\n",
      "          3.18738000e+00, 5.52290332e+00],\n",
      "         [7.97942345e-01, 4.86272395e+00, 1.95737598e+01,\n",
      "          3.72664962e+01, 1.81756581e+02],\n",
      "         [2.68371322e+00, 1.95737598e+01, 2.50215866e+02,\n",
      "          6.70290263e+02, 1.39771241e+03],\n",
      "         [3.18738000e+00, 3.72664962e+01, 6.70290263e+02,\n",
      "          1.07524065e+03, 2.00946196e+03],\n",
      "         [5.52290332e+00, 1.81756581e+02, 1.39771241e+03,\n",
      "          2.00946196e+03, 4.63973348e+03]],\n",
      "\n",
      "        [[3.99019324e-02, 8.47773407e-02, 3.01673171e-01,\n",
      "          3.51464786e-01, 6.65059063e-01],\n",
      "         [8.47773407e-02, 3.40006681e-01, 1.10325027e+00,\n",
      "          2.08564269e+00, 1.16687004e+01],\n",
      "         [3.01673171e-01, 1.10325027e+00, 1.35758469e+01,\n",
      "          3.53818976e+01, 8.70809759e+01],\n",
      "         [3.51464786e-01, 2.08564269e+00, 3.53818976e+01,\n",
      "          6.67905155e+01, 1.47285399e+02],\n",
      "         [6.65059063e-01, 1.16687004e+01, 8.70809759e+01,\n",
      "          1.47285399e+02, 4.41324599e+02]]],\n",
      "\n",
      "\n",
      "       [[[1.16633472e+01, 1.52749260e+01, 3.09894204e+01,\n",
      "          3.96637973e+01, 2.60565502e+02],\n",
      "         [1.52749260e+01, 5.01259885e+01, 1.26024648e+02,\n",
      "          2.33546871e+02, 3.27349157e+03],\n",
      "         [3.09894204e+01, 1.26024648e+02, 5.78238701e+02,\n",
      "          1.69781659e+03, 1.26113649e+04],\n",
      "         [3.96637973e+01, 2.33546871e+02, 1.69781659e+03,\n",
      "          2.77838734e+03, 1.67411419e+04],\n",
      "         [2.60565502e+02, 3.27349157e+03, 1.26113649e+04,\n",
      "          1.67411419e+04, 6.71696610e+05]],\n",
      "\n",
      "        [[9.83610792e+00, 1.46235996e+01, 4.12155671e+01,\n",
      "          3.40235597e+01, 1.88561366e+02],\n",
      "         [1.46235996e+01, 9.55849058e+01, 3.22872572e+02,\n",
      "          4.27339312e+02, 7.81753728e+03],\n",
      "         [4.12155671e+01, 3.22872572e+02, 2.87195164e+03,\n",
      "          6.73848371e+03, 4.50177533e+04],\n",
      "         [3.40235597e+01, 4.27339312e+02, 6.73848371e+03,\n",
      "          7.58999402e+03, 2.74114132e+04],\n",
      "         [1.88561366e+02, 7.81753728e+03, 4.50177533e+04,\n",
      "          2.74114132e+04, 6.09550205e+05]],\n",
      "\n",
      "        [[1.50693115e+00, 4.99024508e+00, 1.24373304e+01,\n",
      "          8.45142386e+00, 2.68890474e+01],\n",
      "         [4.99024508e+00, 9.10530074e+01, 2.48932071e+02,\n",
      "          3.11121544e+02, 5.77843262e+03],\n",
      "         [1.24373304e+01, 2.48932071e+02, 4.92667523e+03,\n",
      "          5.68029569e+03, 1.90657437e+04],\n",
      "         [8.45142386e+00, 3.11121544e+02, 5.68029569e+03,\n",
      "          3.94845034e+03, 9.90036255e+03],\n",
      "         [2.68890474e+01, 5.77843262e+03, 1.90657437e+04,\n",
      "          9.90036255e+03, 5.39769317e+04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[6.30187402e-02, 1.36599063e-01, 6.55356633e-01,\n",
      "          9.25499127e-01, 2.97417604e+00],\n",
      "         [1.36599063e-01, 1.03741054e+00, 4.62048937e+00,\n",
      "          1.13520631e+01, 1.17123693e+02],\n",
      "         [6.55356633e-01, 4.62048937e+00, 9.93119021e+01,\n",
      "          3.29617149e+02, 1.56070659e+03],\n",
      "         [9.25499127e-01, 1.13520631e+01, 3.29617149e+02,\n",
      "          7.83740439e+02, 3.56874955e+03],\n",
      "         [2.97417604e+00, 1.17123693e+02, 1.56070659e+03,\n",
      "          3.56874955e+03, 1.92814009e+04]],\n",
      "\n",
      "        [[6.33397014e-02, 1.42418289e-01, 6.05576173e-01,\n",
      "          8.10896557e-01, 2.42597670e+00],\n",
      "         [1.42418289e-01, 1.39364189e+00, 6.24843597e+00,\n",
      "          1.30972627e+01, 1.51685004e+02],\n",
      "         [6.05576173e-01, 6.24843597e+00, 1.16746209e+02,\n",
      "          3.83779806e+02, 1.45188874e+03],\n",
      "         [8.10896557e-01, 1.30972627e+01, 3.83779806e+02,\n",
      "          7.29206681e+02, 2.72721980e+03],\n",
      "         [2.42597670e+00, 1.51685004e+02, 1.45188874e+03,\n",
      "          2.72721980e+03, 1.38128691e+04]],\n",
      "\n",
      "        [[4.80503543e-02, 7.84714791e-02, 2.95464533e-01,\n",
      "          3.99593255e-01, 1.67565666e+00],\n",
      "         [7.84714791e-02, 2.89461799e-01, 1.16443517e+00,\n",
      "          1.87314090e+00, 1.27143418e+01],\n",
      "         [2.95464533e-01, 1.16443517e+00, 9.01922071e+00,\n",
      "          2.24421423e+01, 1.34472825e+02],\n",
      "         [3.99593255e-01, 1.87314090e+00, 2.24421423e+01,\n",
      "          4.73683475e+01, 2.88914314e+02],\n",
      "         [1.67565666e+00, 1.27143418e+01, 1.34472825e+02,\n",
      "          2.88914314e+02, 2.49972051e+03]]],\n",
      "\n",
      "\n",
      "       [[[2.31077360e-01, 8.35426845e-01, 2.04830858e+00,\n",
      "          2.95125468e+00, 1.01886678e+01],\n",
      "         [8.35426845e-01, 1.45644423e+01, 4.28688705e+01,\n",
      "          1.18969793e+02, 1.44042440e+03],\n",
      "         [2.04830858e+00, 4.28688705e+01, 5.40882032e+02,\n",
      "          1.47886079e+03, 5.09356476e+03],\n",
      "         [2.95125468e+00, 1.18969793e+02, 1.47886079e+03,\n",
      "          2.25878392e+03, 7.60562345e+03],\n",
      "         [1.01886678e+01, 1.44042440e+03, 5.09356476e+03,\n",
      "          7.60562345e+03, 6.24626603e+04]],\n",
      "\n",
      "        [[5.21007246e-01, 1.17701900e+00, 3.45250321e+00,\n",
      "          2.38487029e+00, 7.21382125e+00],\n",
      "         [1.17701900e+00, 2.64430358e+01, 1.00883557e+02,\n",
      "          2.96748218e+02, 3.44373705e+03],\n",
      "         [3.45250321e+00, 1.00883557e+02, 3.77392419e+03,\n",
      "          4.94313056e+03, 1.70819861e+04],\n",
      "         [2.38487029e+00, 2.96748218e+02, 4.94313056e+03,\n",
      "          3.79852834e+03, 1.15548331e+04],\n",
      "         [7.21382125e+00, 3.44373705e+03, 1.70819861e+04,\n",
      "          1.15548331e+04, 5.36315968e+04]],\n",
      "\n",
      "        [[3.30877481e+00, 6.16415690e+00, 1.37017013e+01,\n",
      "          1.33349822e+01, 3.36921742e+01],\n",
      "         [6.16415690e+00, 5.15407812e+01, 1.49132520e+02,\n",
      "          2.57764126e+02, 2.69541800e+03],\n",
      "         [1.37017013e+01, 1.49132520e+02, 1.62403429e+03,\n",
      "          3.47989326e+03, 7.98831630e+03],\n",
      "         [1.33349822e+01, 2.57764126e+02, 3.47989326e+03,\n",
      "          3.10207056e+03, 8.12257678e+03],\n",
      "         [3.36921742e+01, 2.69541800e+03, 7.98831630e+03,\n",
      "          8.12257678e+03, 3.04442716e+04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[1.18915365e-01, 3.02101507e-01, 1.24297412e+00,\n",
      "          1.69152462e+00, 5.10591036e+00],\n",
      "         [3.02101507e-01, 2.12280822e+00, 1.03301618e+01,\n",
      "          1.90316496e+01, 1.65845030e+02],\n",
      "         [1.24297412e+00, 1.03301618e+01, 1.63182430e+02,\n",
      "          3.43027440e+02, 1.71163829e+03],\n",
      "         [1.69152462e+00, 1.90316496e+01, 3.43027440e+02,\n",
      "          7.51416851e+02, 3.42469778e+03],\n",
      "         [5.10591036e+00, 1.65845030e+02, 1.71163829e+03,\n",
      "          3.42469778e+03, 1.90300084e+04]],\n",
      "\n",
      "        [[9.88382537e-02, 2.86176746e-01, 1.02736071e+00,\n",
      "          1.50335665e+00, 4.37266170e+00],\n",
      "         [2.86176746e-01, 2.63367689e+00, 1.10723584e+01,\n",
      "          2.68965298e+01, 2.06920328e+02],\n",
      "         [1.02736071e+00, 1.10723584e+01, 1.22885504e+02,\n",
      "          4.36417117e+02, 1.60125354e+03],\n",
      "         [1.50335665e+00, 2.68965298e+01, 4.36417117e+02,\n",
      "          9.47564201e+02, 2.88422761e+03],\n",
      "         [4.37266170e+00, 2.06920328e+02, 1.60125354e+03,\n",
      "          2.88422761e+03, 1.70883054e+04]],\n",
      "\n",
      "        [[3.36173281e-02, 5.43447436e-02, 2.56537248e-01,\n",
      "          3.65197846e-01, 9.94142159e-01],\n",
      "         [5.43447436e-02, 1.91094769e-01, 8.70528143e-01,\n",
      "          1.42901605e+00, 6.21185202e+00],\n",
      "         [2.56537248e-01, 8.70528143e-01, 7.64909483e+00,\n",
      "          2.38513661e+01, 1.08404030e+02],\n",
      "         [3.65197846e-01, 1.42901605e+00, 2.38513661e+01,\n",
      "          6.07471129e+01, 1.78502677e+02],\n",
      "         [9.94142159e-01, 6.21185202e+00, 1.08404030e+02,\n",
      "          1.78502677e+02, 7.91834758e+02]]]])>, <tf.Tensor: shape=(11,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])>)\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_8 (Conv2D)           (None, 59, 59, 16)        8800      \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 19, 19, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_8 (Flatten)         (None, 5776)              0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 512)               2957824   \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,098,209\n",
      "Trainable params: 3,098,209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 61, 61, 61), found shape=(None, 61, 5, 5)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m training_time\u001b[39m.\u001b[39mloc[lag, \u001b[39m'\u001b[39m\u001b[39mCPU_Time\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fold)] \u001b[39m=\u001b[39m cpu_time\n\u001b[0;32m     43\u001b[0m training_time\u001b[39m.\u001b[39mloc[lag, \u001b[39m'\u001b[39m\u001b[39mWall_Time\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fold)] \u001b[39m=\u001b[39m wall_time\n\u001b[1;32m---> 45\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(test_ds, callbacks \u001b[39m=\u001b[39;49m myCallbacks(log_path))\n\u001b[0;32m     47\u001b[0m recap\u001b[39m.\u001b[39mloc[lag, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fold)] \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     48\u001b[0m recap\u001b[39m.\u001b[39mloc[lag, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(fold)] \u001b[39m=\u001b[39m results[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileex72e_c8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\training.py\", line 1727, in test_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\training.py\", line 1713, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\training.py\", line 1701, in run_step  **\n        outputs = model.test_step(data)\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\training.py\", line 1665, in test_step\n        y_pred = self(x, training=False)\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\fulky\\anaconda3\\envs\\skripsi\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 61, 61, 61), found shape=(None, 61, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "log_dirs = ['logs/CNN_Bispec2D','logs1/CNN_Bispec2D', 'logs2/CNN_Bispec2D', 'logs3/CNN_Bispec2D', 'logs4/CNN_Bispec2D', 'logs5/CNN_Bispec2D', 'logs6/CNN_Bispec2D', 'logs7/CNN_Bispec2D', 'logs8/CNN_Bispec2D', 'logs9/CNN_Bispec2D']\n",
    "train_dir = '../Feature/Bispectrum2D/Train/smni_cmi_train_bispectrum2d'\n",
    "test_dir = '../Feature/Bispectrum2D/Test/smni_cmi_test_bispectrum2d'\n",
    "\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured) \n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_bispectrum2d(train_temp_dir)\n",
    "            test_ds = get_batch_bispectrum2d(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_CNN_bispectrum2d([i[0].shape[1] for i in train][0])\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path), verbose = 0)\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        print('test')\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = ['logs/ANN_Bispec2D','logs1/ANN_Bispec2D', 'logs2/ANN_Bispec2D', 'logs3/ANN_Bispec2D', 'logs4/ANN_Bispec2D', 'logs5/ANN_Bispec2D', 'logs6/ANN_Bispec2D', 'logs7/ANN_Bispec2D', 'logs8/ANN_Bispec2D', 'logs9/ANN_Bispec2D']\n",
    "train_dir = '../Feature/Bispectrum2D/Train/smni_cmi_train_bispectrum2d'\n",
    "test_dir = '../Feature/Bispectrum2D/Test/smni_cmi_test_bispectrum2d'\n",
    "\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured) \n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_bispectrum2d(train_temp_dir)\n",
    "            test_ds = get_batch_bispectrum2d(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_ANN_bispectrum2d([i[0].shape[1] for i in train][0])\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path), verbose = 0)\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        print('test')\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bispectrum 2D Flat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = ['logs/CNN_Bispec2DFlat','logs1/CNN_Bispec2DFlat', 'logs2/CNN_Bispec2DFlat', 'logs3/CNN_Bispec2DFlat', 'logs4/CNN_Bispec2DFlat', 'logs5/CNN_Bispec2DFlat', 'logs6/CNN_Bispec2DFlat', 'logs7/CNN_Bispec2DFlat', 'logs8/CNN_Bispec2DFlat', 'logs9/CNN_Bispec2DFlat']\n",
    "train_dir = '../Feature/Bispectrum2DFlat/Train/smni_cmi_train_bispectrum2dflat'\n",
    "test_dir = '../Feature/Bispectrum2DFlat/Test/smni_cmi_test_bispectrum2dflat'\n",
    "\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured) \n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_bispectrum2d_flat(train_temp_dir)\n",
    "            test_ds = get_batch_bispectrum2d_flat(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_CNN_bispectrum2d_flat([i[0].shape[1] for i in train][0])\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path), verbose = 0)\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        print('test')\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dirs = ['logs/ANN_Bispec2DFlat','logs1/ANN_Bispec2DFlat', 'logs2/ANN_Bispec2DFlat', 'logs3/ANN_Bispec2DFlat', 'logs4/ANN_Bispec2DFlat', 'logs5/ANN_Bispec2DFlat', 'logs6/ANN_Bispec2DFlat', 'logs7/ANN_Bispec2DFlat', 'logs8/ANN_Bispec2DFlat', 'logs9/ANN_Bispec2DFlat']\n",
    "train_dir = '../Feature/Bispectrum2DFlat/Train/smni_cmi_train_bispectrum2dflat'\n",
    "test_dir = '../Feature/Bispectrum2DFlat/Test/smni_cmi_test_bispectrum2dflat'\n",
    "\n",
    "for log_dir in log_dirs:\n",
    "    recap = pd.DataFrame(index=lags, columns=folds)\n",
    "    training_time = pd.DataFrame(index=lags, columns=time_measured) \n",
    "    for fold in range(1,3):\n",
    "        for lag in lags:\n",
    "            if fold == 2:\n",
    "                train_dir, test_dir = test_dir, train_dir\n",
    "            \n",
    "            train_temp_dir = train_dir + '_' + str(lag)\n",
    "            test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "            train = get_batch_bispectrum2d_flat(train_temp_dir)\n",
    "            test_ds = get_batch_bispectrum2d_flat(test_temp_dir)\n",
    "\n",
    "            train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "            train_ds = train.take(train_size)\n",
    "            val_ds = train.skip(train_size)\n",
    "\n",
    "            log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "            model = create_model_ANN_bispectrum2d_flat([i[0].shape[1] for i in train][0])\n",
    "            model.summary()\n",
    "\n",
    "            model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "            cpu_start = time.process_time()\n",
    "            wt_start = time.time()\n",
    "\n",
    "            history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path), verbose = 0)\n",
    "\n",
    "            wt_end = time.time()\n",
    "            cpu_end = time.process_time()\n",
    "            wall_time = wt_end - wt_start\n",
    "            cpu_time = cpu_end - cpu_start\n",
    "            training_time.loc[lag, 'CPU_Time'+ '_' + str(fold)] = cpu_time\n",
    "            training_time.loc[lag, 'Wall_Time'+ '_' + str(fold)] = wall_time\n",
    "\n",
    "            results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "            recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc']\n",
    "            recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "            recap.loc[lag, 'epoch'+ '_' + str(fold)] = len(history.history['acc'])\n",
    "    log_dir = os.path.join(log_dir,'Recap')\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        print('test')\n",
    "    recap.to_csv(os.path.join(log_dir,'recap.csv'))\n",
    "    training_time.to_csv(os.path.join(log_dir,'Training_time.csv'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

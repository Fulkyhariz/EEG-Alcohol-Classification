{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing, model_selection\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(directory):\n",
    "    data = pd.DataFrame(columns=['data', 'label'])\n",
    "    for foldername in os.listdir(directory):\n",
    "        folder = os.path.join(directory, foldername)\n",
    "        if os.path.isdir(folder):\n",
    "            files = os.listdir(folder)\n",
    "            for filename in files:\n",
    "                rel_path = os.path.join(directory, foldername, filename)\n",
    "                temp_label = filename.split('.')[0].split('_')[0]\n",
    "                if 'a' in temp_label:\n",
    "                    label ='alcoholic'\n",
    "                else:\n",
    "                    label = 'control'\n",
    "\n",
    "                temp_data = pd.DataFrame(columns=['data', 'label'], index=[0])\n",
    "\n",
    "                rwb = np.load(rel_path)\n",
    "                rwb.astype(np.float64).reshape(-1,1)\n",
    "                # print(rwb)\n",
    "                # with open(rel_path, 'r') as file:\n",
    "                    \n",
    "                #     rwb = list(csv.reader(file, delimiter=\",\"))[0]\n",
    "                #     # scaler = preprocessing.MinMaxScaler()\n",
    "                #     rwb = np.asarray(rwb).astype(np.float64).reshape(-1,1)\n",
    "                #     # print(rwb)\n",
    "                                \n",
    "                temp_data['data'][0] = rwb\n",
    "                temp_data['label'] = label\n",
    "                \n",
    "                # decomp = np.arange(0, 366)\n",
    "                # plt.plot(decomp, df_data)\n",
    "                # plt.xlabel('Dimension Number')\n",
    "                # plt.ylabel('Wavelet Bispectrum Energy')\n",
    "                # plt.show()\n",
    "                data = pd.concat([data, temp_data], ignore_index=True)\n",
    "    label_map = {\"alcoholic\": 1, \"control\": 0}\n",
    "    data['label_map'] = data['label'].map(label_map)\n",
    "    # print(data)      \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(path):\n",
    "    # loading extracted feature & label\n",
    "    x = get_dataset(path)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "    series_list = [\n",
    "        i for i in x[\"data\"]\n",
    "    ]\n",
    "\n",
    "    # series_list = series_list.reshape(-1, 366, 1)\n",
    "\n",
    "    labels_list = [i for i in x[\"label_map\"]]\n",
    "        \n",
    "    # y = keras.utils.to_categorical(y[0])\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((series_list,labels_list))\n",
    "    dataset = dataset.shuffle(len(labels_list)).batch(32)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_batch('../smni_cmi_test_bispectrum_256')\n",
    "for i in test:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "signal = np.load('../smni_cmi_test_bispectrum_4\\co2a0000364\\co2a0000364_30_bispectrum.npy')\n",
    "\n",
    "print(signal)\n",
    "# Extract the signal values from the DataFrame\n",
    "\n",
    "# Create a time axis for the signal\n",
    "t = range(len(signal))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# Plot the signal\n",
    "ax.plot(t, signal)\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Signal amplitude')\n",
    "ax.set_title('Signal plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(layers.Input(shape=(915,)))\n",
    "    model.add(layers.Reshape((915, 1)))\n",
    "\n",
    "    model.add(layers.Conv1D(filters=16, kernel_size=4, activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling1D(pool_size=4))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Conv1D(filters=8, kernel_size=2, activation=\"relu\"))\n",
    "    model.add(layers.MaxPooling1D(pool_size=4))\n",
    "    model.add(layers.BatchNormalization())\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    model.add(layers.Dense(512, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCallbacks(log_dir):\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    return tensorboard_callback\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = [256, 128, 64, 32, 16, 8, 4, 2]\n",
    "folds = ['train_1', 'test_1', 'train_2', 'test_2']\n",
    "epochs = 100\n",
    "log_dir = 'logs'\n",
    "train_dir = '../smni_cmi_train_bispectrum'\n",
    "test_dir = '../smni_cmi_train_bispectrum'\n",
    "\n",
    "recap = pd.DataFrame(index=lags, columns=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_1 (Reshape)         (None, 915, 1)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 912, 16)           80        \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 228, 16)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 228, 16)          64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 227, 8)            264       \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 56, 8)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 56, 8)            32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 448)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               229888    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361,913\n",
      "Trainable params: 361,865\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 332ms/step - loss: 0.6730 - acc: 0.5938 - val_loss: 0.1214 - val_acc: 1.0000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 103ms/step - loss: 0.1304 - acc: 1.0000 - val_loss: 0.0466 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 91ms/step - loss: 0.0402 - acc: 1.0000 - val_loss: 0.1294 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.0796 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0351 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.3448e-04 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.0778e-05 - acc: 1.0000 - val_loss: 0.0428 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 6.6704e-05 - acc: 1.0000 - val_loss: 0.0991 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 2.0703e-05 - acc: 1.0000 - val_loss: 0.0407 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 4.6747e-06 - acc: 1.0000 - val_loss: 0.0454 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 4.9947e-07 - acc: 1.0000 - val_loss: 0.0216 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 6.7901e-06 - acc: 1.0000 - val_loss: 0.0324 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.2804e-06 - acc: 1.0000 - val_loss: 0.0245 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 6.8791e-07 - acc: 1.0000 - val_loss: 0.0211 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.4612e-07 - acc: 1.0000 - val_loss: 0.0128 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 3.1252e-07 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 9.3741e-08 - acc: 1.0000 - val_loss: 0.0069 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3.1467e-07 - acc: 1.0000 - val_loss: 0.0144 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.2125e-07 - acc: 1.0000 - val_loss: 0.0089 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 4.9168e-07 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 5.3319e-08 - acc: 1.0000 - val_loss: 0.0037 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.1739e-07 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 1.2344e-08 - acc: 1.0000 - val_loss: 0.0052 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.5348e-08 - acc: 1.0000 - val_loss: 0.0054 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 3.1231e-09 - acc: 1.0000 - val_loss: 5.3784e-04 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 3.6001e-08 - acc: 1.0000 - val_loss: 4.2338e-04 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.2813e-07 - acc: 1.0000 - val_loss: 6.3445e-04 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 2.6978e-08 - acc: 1.0000 - val_loss: 2.8837e-04 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 4.3810e-08 - acc: 1.0000 - val_loss: 8.7935e-04 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.9601e-08 - acc: 1.0000 - val_loss: 1.8074e-04 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 4.2021e-09 - acc: 1.0000 - val_loss: 1.0541e-04 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 4.7207e-07 - acc: 1.0000 - val_loss: 4.7306e-04 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0792e-07 - acc: 1.0000 - val_loss: 2.0219e-04 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.1782e-08 - acc: 1.0000 - val_loss: 3.1162e-04 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 4.6855e-08 - acc: 1.0000 - val_loss: 2.4335e-04 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.4126e-08 - acc: 1.0000 - val_loss: 9.4015e-05 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 8.0223e-08 - acc: 1.0000 - val_loss: 1.2891e-04 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.0641e-07 - acc: 1.0000 - val_loss: 3.9138e-05 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.3526e-08 - acc: 1.0000 - val_loss: 1.2342e-04 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.2227e-07 - acc: 1.0000 - val_loss: 8.2935e-05 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2.7095e-08 - acc: 1.0000 - val_loss: 1.2570e-04 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 6.9645e-08 - acc: 1.0000 - val_loss: 1.8462e-05 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.0753e-07 - acc: 1.0000 - val_loss: 6.8897e-05 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.6241e-08 - acc: 1.0000 - val_loss: 1.2240e-04 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 5.5991e-08 - acc: 1.0000 - val_loss: 6.5075e-05 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.2796e-07 - acc: 1.0000 - val_loss: 8.2465e-05 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 2.9345e-07 - acc: 1.0000 - val_loss: 2.3444e-05 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 1.1171e-07 - acc: 1.0000 - val_loss: 5.0428e-05 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 6.6067e-08 - acc: 1.0000 - val_loss: 2.9603e-05 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 5.6514e-08 - acc: 1.0000 - val_loss: 6.8118e-05 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.6306e-07 - acc: 1.0000 - val_loss: 2.0653e-05 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.3854e-08 - acc: 1.0000 - val_loss: 4.1381e-06 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.1836e-07 - acc: 1.0000 - val_loss: 9.9734e-06 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.5987e-07 - acc: 1.0000 - val_loss: 1.6142e-06 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 5.7236e-08 - acc: 1.0000 - val_loss: 6.3815e-06 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.5455e-07 - acc: 1.0000 - val_loss: 3.8715e-06 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 6.9331e-08 - acc: 1.0000 - val_loss: 3.4674e-06 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.7275e-08 - acc: 1.0000 - val_loss: 4.9309e-06 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 2.1795e-08 - acc: 1.0000 - val_loss: 4.8410e-06 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.4091e-07 - acc: 1.0000 - val_loss: 2.8505e-06 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.7011e-08 - acc: 1.0000 - val_loss: 5.5561e-06 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.8639e-08 - acc: 1.0000 - val_loss: 1.1448e-06 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 56ms/step - loss: 7.6556e-08 - acc: 1.0000 - val_loss: 8.2768e-06 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 3.2559e-07 - acc: 1.0000 - val_loss: 1.6116e-06 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 2.3652e-08 - acc: 1.0000 - val_loss: 3.7939e-06 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.7290e-08 - acc: 1.0000 - val_loss: 2.4943e-06 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 2.9559e-08 - acc: 1.0000 - val_loss: 2.5784e-06 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.5580e-08 - acc: 1.0000 - val_loss: 1.9361e-06 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.4663e-07 - acc: 1.0000 - val_loss: 4.3982e-07 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 57ms/step - loss: 4.2857e-08 - acc: 1.0000 - val_loss: 1.7054e-06 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 2.3729e-07 - acc: 1.0000 - val_loss: 1.7221e-06 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 4.6656e-08 - acc: 1.0000 - val_loss: 2.2684e-06 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 2.6457e-08 - acc: 1.0000 - val_loss: 1.5550e-05 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 1.9877e-07 - acc: 1.0000 - val_loss: 1.7166e-06 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.3411e-08 - acc: 1.0000 - val_loss: 8.7212e-07 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 6.6003e-09 - acc: 1.0000 - val_loss: 1.7389e-06 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.3819e-08 - acc: 1.0000 - val_loss: 1.7134e-06 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.4467e-09 - acc: 1.0000 - val_loss: 1.0698e-05 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 7.6484e-10 - acc: 1.0000 - val_loss: 7.6573e-07 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 5.5561e-08 - acc: 1.0000 - val_loss: 1.8779e-06 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.8776e-07 - acc: 1.0000 - val_loss: 6.3694e-07 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 3.3549e-07 - acc: 1.0000 - val_loss: 1.1950e-06 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.4358e-07 - acc: 1.0000 - val_loss: 9.5373e-07 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 3.5311e-08 - acc: 1.0000 - val_loss: 8.3105e-06 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.3858e-07 - acc: 1.0000 - val_loss: 2.6683e-07 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 2.3231e-08 - acc: 1.0000 - val_loss: 3.5387e-07 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 1.8199e-08 - acc: 1.0000 - val_loss: 6.5073e-07 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.0786e-07 - acc: 1.0000 - val_loss: 8.7344e-07 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.2658e-07 - acc: 1.0000 - val_loss: 6.2736e-06 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 2.4179e-08 - acc: 1.0000 - val_loss: 1.3273e-06 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.0660e-10 - acc: 1.0000 - val_loss: 4.7602e-07 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 3.6168e-08 - acc: 1.0000 - val_loss: 5.9814e-06 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 88ms/step - loss: 7.3457e-08 - acc: 1.0000 - val_loss: 1.2003e-06 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.4176e-07 - acc: 1.0000 - val_loss: 8.1287e-07 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 5.1563e-08 - acc: 1.0000 - val_loss: 5.5151e-07 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 6.9095e-08 - acc: 1.0000 - val_loss: 2.2594e-07 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 70ms/step - loss: 1.0631e-08 - acc: 1.0000 - val_loss: 5.3281e-07 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 9.6493e-09 - acc: 1.0000 - val_loss: 1.1594e-06 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 59ms/step - loss: 1.9839e-08 - acc: 1.0000 - val_loss: 5.2201e-07 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 4.1829e-08 - acc: 1.0000 - val_loss: 3.8504e-06 - val_acc: 1.0000\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.1055e-06 - acc: 1.0000\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape_2 (Reshape)         (None, 915, 1)            0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 912, 16)           80        \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPooling  (None, 228, 16)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 228, 16)          64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 227, 8)            264       \n",
      "                                                                 \n",
      " max_pooling1d_5 (MaxPooling  (None, 56, 8)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 56, 8)            32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 448)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               229888    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 361,913\n",
      "Trainable params: 361,865\n",
      "Non-trainable params: 48\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 262ms/step - loss: 0.4306 - acc: 0.8281 - val_loss: 0.0891 - val_acc: 1.0000\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.1357 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 1.0000\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 86ms/step - loss: 0.0171 - acc: 1.0000 - val_loss: 0.0431 - val_acc: 1.0000\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 78ms/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0519 - val_acc: 1.0000\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.7206e-04 - acc: 1.0000 - val_loss: 0.0808 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 8.9849e-05 - acc: 1.0000 - val_loss: 0.0717 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 5.4978e-06 - acc: 1.0000 - val_loss: 0.1165 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.6014e-05 - acc: 1.0000 - val_loss: 0.0507 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 3.7345e-07 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.3736e-06 - acc: 1.0000 - val_loss: 0.0737 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 93ms/step - loss: 3.4380e-07 - acc: 1.0000 - val_loss: 0.0602 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 8.1098e-07 - acc: 1.0000 - val_loss: 0.0489 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.1699e-07 - acc: 1.0000 - val_loss: 0.0542 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.2306e-09 - acc: 1.0000 - val_loss: 0.0464 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.3339e-07 - acc: 1.0000 - val_loss: 0.0373 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 74ms/step - loss: 7.4992e-08 - acc: 1.0000 - val_loss: 0.0376 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.4621e-09 - acc: 1.0000 - val_loss: 0.0234 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.3281e-08 - acc: 1.0000 - val_loss: 0.0246 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 7.7341e-09 - acc: 1.0000 - val_loss: 0.0121 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 3.6293e-08 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.5727e-08 - acc: 1.0000 - val_loss: 0.0108 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.0316e-08 - acc: 1.0000 - val_loss: 0.0078 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 7.2884e-09 - acc: 1.0000 - val_loss: 0.0062 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.8513e-08 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 5.6698e-09 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 7.6010e-08 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 80ms/step - loss: 2.1293e-09 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 5.4940e-08 - acc: 1.0000 - val_loss: 0.0050 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.3035e-08 - acc: 1.0000 - val_loss: 5.7662e-04 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 3.9915e-08 - acc: 1.0000 - val_loss: 6.6229e-04 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 90ms/step - loss: 3.3023e-09 - acc: 1.0000 - val_loss: 2.1340e-04 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.8826e-08 - acc: 1.0000 - val_loss: 8.0968e-04 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3.9111e-10 - acc: 1.0000 - val_loss: 4.8268e-04 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 71ms/step - loss: 8.0018e-10 - acc: 1.0000 - val_loss: 9.6313e-05 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 4.4000e-08 - acc: 1.0000 - val_loss: 1.8738e-04 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.1062e-08 - acc: 1.0000 - val_loss: 4.5425e-04 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 8.5550e-10 - acc: 1.0000 - val_loss: 3.2435e-04 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 1.1946e-08 - acc: 1.0000 - val_loss: 1.7488e-04 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 5.6954e-09 - acc: 1.0000 - val_loss: 1.1850e-04 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 3.0111e-09 - acc: 1.0000 - val_loss: 1.7519e-04 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.3754e-10 - acc: 1.0000 - val_loss: 1.9159e-04 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 1.1678e-08 - acc: 1.0000 - val_loss: 9.6166e-05 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 1.9034e-08 - acc: 1.0000 - val_loss: 1.3568e-04 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 4.7663e-09 - acc: 1.0000 - val_loss: 1.3449e-04 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 4.2449e-09 - acc: 1.0000 - val_loss: 1.9777e-05 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 2.1891e-08 - acc: 1.0000 - val_loss: 3.2481e-05 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 7.5059e-08 - acc: 1.0000 - val_loss: 1.3033e-05 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.9911e-09 - acc: 1.0000 - val_loss: 3.0810e-05 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.8586e-09 - acc: 1.0000 - val_loss: 1.8691e-05 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 3.2472e-08 - acc: 1.0000 - val_loss: 4.7308e-06 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 7.7688e-09 - acc: 1.0000 - val_loss: 5.1864e-06 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 7.7077e-09 - acc: 1.0000 - val_loss: 4.7842e-05 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 4.7470e-09 - acc: 1.0000 - val_loss: 2.7513e-06 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.0386e-08 - acc: 1.0000 - val_loss: 9.1256e-06 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 2.4384e-09 - acc: 1.0000 - val_loss: 1.2704e-05 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 5.5043e-09 - acc: 1.0000 - val_loss: 7.4614e-06 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 1.5033e-08 - acc: 1.0000 - val_loss: 2.1293e-06 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 2.5061e-09 - acc: 1.0000 - val_loss: 1.1121e-05 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.8899e-09 - acc: 1.0000 - val_loss: 1.3189e-05 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 2.0445e-09 - acc: 1.0000 - val_loss: 4.3862e-06 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 2.0503e-08 - acc: 1.0000 - val_loss: 8.6506e-06 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 1.9767e-08 - acc: 1.0000 - val_loss: 1.9214e-05 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 2.6768e-11 - acc: 1.0000 - val_loss: 2.0019e-06 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 5.8127e-08 - acc: 1.0000 - val_loss: 1.2075e-06 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3.0634e-09 - acc: 1.0000 - val_loss: 3.6468e-07 - val_acc: 1.0000\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 72ms/step - loss: 6.0495e-09 - acc: 1.0000 - val_loss: 3.1163e-07 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 2.8144e-11 - acc: 1.0000 - val_loss: 3.2776e-06 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 4.6401e-09 - acc: 1.0000 - val_loss: 3.5915e-07 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 4.0591e-09 - acc: 1.0000 - val_loss: 2.8784e-06 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 9.3912e-09 - acc: 1.0000 - val_loss: 1.2418e-06 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 7.5382e-08 - acc: 1.0000 - val_loss: 1.8473e-06 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 67ms/step - loss: 1.3472e-08 - acc: 1.0000 - val_loss: 6.0702e-07 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 3.2730e-09 - acc: 1.0000 - val_loss: 3.4333e-07 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 3.5776e-10 - acc: 1.0000 - val_loss: 9.9319e-08 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 5.4948e-09 - acc: 1.0000 - val_loss: 9.2431e-07 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 63ms/step - loss: 5.4511e-09 - acc: 1.0000 - val_loss: 1.6345e-06 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 1.7469e-08 - acc: 1.0000 - val_loss: 3.8953e-07 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 73ms/step - loss: 1.4458e-09 - acc: 1.0000 - val_loss: 1.5683e-07 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 62ms/step - loss: 2.5941e-08 - acc: 1.0000 - val_loss: 3.4962e-07 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 3.9806e-09 - acc: 1.0000 - val_loss: 6.5191e-07 - val_acc: 1.0000\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 2.1587e-12 - acc: 1.0000 - val_loss: 2.2782e-06 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 68ms/step - loss: 8.2432e-10 - acc: 1.0000 - val_loss: 1.6111e-07 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 9.2678e-09 - acc: 1.0000 - val_loss: 5.6360e-08 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 4.4785e-09 - acc: 1.0000 - val_loss: 1.2264e-07 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 3.5380e-09 - acc: 1.0000 - val_loss: 1.1123e-06 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 58ms/step - loss: 4.0370e-09 - acc: 1.0000 - val_loss: 1.1259e-07 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 2.9235e-09 - acc: 1.0000 - val_loss: 9.1618e-08 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 61ms/step - loss: 8.7867e-10 - acc: 1.0000 - val_loss: 1.4350e-08 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 2.6115e-08 - acc: 1.0000 - val_loss: 7.7010e-08 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 76ms/step - loss: 4.1601e-09 - acc: 1.0000 - val_loss: 2.2865e-08 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 1.7341e-10 - acc: 1.0000 - val_loss: 2.9995e-07 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 66ms/step - loss: 1.1115e-08 - acc: 1.0000 - val_loss: 7.5354e-08 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 65ms/step - loss: 5.6255e-09 - acc: 1.0000 - val_loss: 3.8228e-07 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 64ms/step - loss: 5.1379e-10 - acc: 1.0000 - val_loss: 3.2001e-07 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 97ms/step - loss: 1.9140e-07 - acc: 1.0000 - val_loss: 3.7377e-07 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 4.7418e-08 - acc: 1.0000 - val_loss: 1.2765e-08 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 77ms/step - loss: 5.3246e-09 - acc: 1.0000 - val_loss: 1.1447e-08 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 81ms/step - loss: 1.8860e-09 - acc: 1.0000 - val_loss: 5.2902e-08 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 60ms/step - loss: 4.7043e-12 - acc: 1.0000 - val_loss: 9.2364e-08 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 69ms/step - loss: 3.6059e-09 - acc: 1.0000 - val_loss: 1.6395e-07 - val_acc: 1.0000\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 6.0052e-08 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "for fold in range(1,3):\n",
    "    for lag in [256]:\n",
    "        if fold == 2:\n",
    "            train_dir, test_dir = test_dir, train_dir\n",
    "        \n",
    "        train_temp_dir = train_dir + '_' + str(lag)\n",
    "        test_temp_dir = test_dir + '_' + str(lag)\n",
    "\n",
    "        train = get_batch('../smni_cmi_test_bispectrum_256')\n",
    "        test_ds = get_batch('../smni_cmi_test_bispectrum_256')\n",
    "\n",
    "        train_size = int(len(list(train.as_numpy_iterator()))*0.8)\n",
    "        train_ds = train.take(train_size)\n",
    "        val_ds = train.skip(train_size)\n",
    "\n",
    "        log_path = os.path.join(log_dir, str(fold), str(lag))\n",
    "\n",
    "        model = create_model()\n",
    "        model.summary()\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])\n",
    "\n",
    "        history = model.fit(train_ds, epochs=epochs, validation_data=(val_ds), callbacks = myCallbacks(log_path))\n",
    "        results = model.evaluate(test_ds, callbacks = myCallbacks(log_path))\n",
    "\n",
    "        recap.loc[lag, 'train'+ '_' + str(fold)] = history.history['acc'][-1]\n",
    "        recap.loc[lag, 'test'+ '_' + str(fold)] = results[1]\n",
    "\n",
    "        model.save(os.path.join(log_path,'model.h5'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recap.to_csv('../Logs/Recap/recap.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir logs --port=8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
